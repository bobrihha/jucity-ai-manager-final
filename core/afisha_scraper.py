"""–ü–∞—Ä—Å–µ—Ä –∞—Ñ–∏—à–∏ —Å —Å–∞–π—Ç–∞ jucity.ru"""

import requests
from bs4 import BeautifulSoup
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

AFISHA_URL = "https://nn.jucity.ru/afisha/"

def scrape_afisha() -> str:
    """–ü–∞—Ä—Å–∏—Ç –∞—Ñ–∏—à—É —Å —Å–∞–π—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ–±—ã—Ç–∏–π."""
    try:
        response = requests.get(AFISHA_URL, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        events = soup.select('.events__item')
        
        if not events:
            return "–ê—Ñ–∏—à–∞: –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –Ω–µ—Ç –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π."
        
        result_lines = ["–ê–§–ò–®–ê –î–ñ–£–ù–ì–õ–ò –°–ò–¢–ò (–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥)\n"]
        result_lines.append(f"–ê–∫—Ç—É–∞–ª—å–Ω–æ –Ω–∞ –º–æ–º–µ–Ω—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n")
        result_lines.append("-" * 40 + "\n")
        
        for event in events:
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
            title_el = event.select_one('.events__item-title a')
            title = title_el.get_text(strip=True) if title_el else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è
            info_rows = event.select('.events__item-info-row')
            date = ""
            time = ""
            
            for row in info_rows:
                text = row.get_text(strip=True)
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Ñ–æ—Ä–º–∞—Ç—É: –¥–∞—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫–∏, –≤—Ä–µ–º—è - –¥–≤–æ–µ—Ç–æ—á–∏–µ
                if '.' in text and len(text) <= 10:
                    date = text
                elif ':' in text and len(text) <= 5:
                    time = text
            
            # –°—Å—ã–ª–∫–∞
            link_el = event.select_one('.events__item-link a')
            link = link_el.get('href', '') if link_el else ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            result_lines.append(f"üìÖ {date} –≤ {time}")
            result_lines.append(f"üé™ {title}")
            if link:
                result_lines.append(f"–ü–æ–¥—Ä–æ–±–Ω–µ–µ: {link}")
            result_lines.append("")
        
        result_lines.append("-" * 40)
        result_lines.append(f"–ü–æ–ª–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ: {AFISHA_URL}")
        
        return "\n".join(result_lines)
        
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∞—Ñ–∏—à–∏: {e}")
        return f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞—Ñ–∏—à—É. –°–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ —Å–∞–π—Ç–µ: {AFISHA_URL}"


def save_afisha_to_knowledge(park_id: str = "nn") -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—Ñ–∏—à—É –≤ —Ñ–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
    content = scrape_afisha()
    
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∞—Ñ–∏—à–∏
    knowledge_dir = Path(__file__).parent.parent / "knowledge" / park_id / "events"
    knowledge_dir.mkdir(parents=True, exist_ok=True)
    
    afisha_file = knowledge_dir / "afisha.txt"
    afisha_file.write_text(content, encoding="utf-8")
    
    logger.info(f"–ê—Ñ–∏—à–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {afisha_file}")
    return content


if __name__ == "__main__":
    # –¢–µ—Å—Ç
    print(scrape_afisha())
